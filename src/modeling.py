import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf
import os
import warnings
warnings.filterwarnings('ignore')

# ============================================================
# RANDOM SEED - Äáº£m báº£o káº¿t quáº£ láº·p láº¡i Ä‘Æ°á»£c (Reproducibility)
# ============================================================
RANDOM_SEED = 42

np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)
os.environ['TF_DETERMINISTIC_OPS'] = '1'
os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)


def evaluate_returns_metrics(y_true_returns, y_pred_returns, model_name):
    """
    ÄÃ¡nh giÃ¡ metrics cho dá»± bÃ¡o Log Returns
    
    Táº¡i sao khÃ¡c vá»›i dá»± bÃ¡o giÃ¡:
    - RÂ² trÃªn returns thÆ°á»ng THáº¤P (0.01-0.15) nhÆ°ng Ä‘Ã¢y lÃ  BÃŒN THÆ¯á»œNG vá»›i dá»¯ liá»‡u tÃ i chÃ­nh
    - Direction Accuracy > 55% Ä‘Ã£ lÃ  cÃ³ giÃ¡ trá»‹ thÆ°Æ¡ng máº¡i
    - RMSE/MAE Ä‘o trÃªn returns (scale nhá» hÆ¡n nhiá»u so vá»›i giÃ¡)
    
    Metrics:
    - RMSE: Root Mean Squared Error - Ä‘o sai sá»‘ trung bÃ¬nh
    - MAE: Mean Absolute Error - sai sá»‘ tuyá»‡t Ä‘á»‘i trung bÃ¬nh
    - RÂ²: Coefficient of Determination - tá»· lá»‡ variance Ä‘Æ°á»£c giáº£i thÃ­ch
    - Direction Accuracy: % dá»± Ä‘oÃ¡n Ä‘Ãºng chiá»u hÆ°á»›ng (lÃªn/xuá»‘ng)
      â†’ ÄÃ¢y lÃ  metric QUAN TRá»ŒNG NHáº¤T cho trading!
    """
    rmse = np.sqrt(mean_squared_error(y_true_returns, y_pred_returns))
    mae = mean_absolute_error(y_true_returns, y_pred_returns)
    r2 = r2_score(y_true_returns, y_pred_returns)
    
    # Direction Accuracy - Tá»· lá»‡ dá»± Ä‘oÃ¡n Ä‘Ãºng chiá»u hÆ°á»›ng
    # Náº¿u cáº£ actual vÃ  predicted cÃ¹ng dáº¥u (+/+) hoáº·c (-/-) â†’ ÄÃºng chiá»u hÆ°á»›ng
    correct_direction = np.sum(np.sign(y_true_returns) == np.sign(y_pred_returns))
    direction_accuracy = (correct_direction / len(y_true_returns)) * 100
    
    print(f"\n{'â”€'*70}")
    print(f"METRICS - {model_name} (Log Returns Prediction)")
    print(f"{'â”€'*70}")
    print(f"RMSE:                  {rmse:.6f}")
    print(f"MAE:                   {mae:.6f}")
    print(f"RÂ²:                    {r2:.4f}")
    print(f"Direction Accuracy:    {direction_accuracy:.2f}%")
    print(f"{'â”€'*70}")
    
    # Nháº­n xÃ©t
    print(f"\nğŸ“Š NHáº¬N XÃ‰T:")
    if r2 < 0:
        print(f"   âš  RÂ² Ã¢m: MÃ´ hÃ¬nh KÃ‰ME hÆ¡n cáº£ viá»‡c dá»± Ä‘oÃ¡n mean constant")
    elif r2 < 0.05:
        print(f"   â„¹ RÂ² tháº¥p ({r2:.4f}) nhÆ°ng BÃŒN THÆ¯á»œNG vá»›i dá»¯ liá»‡u tÃ i chÃ­nh")
    else:
        print(f"   âœ“ RÂ² = {r2:.4f}: MÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng giáº£i thÃ­ch {r2*100:.2f}% variance")
    
    if direction_accuracy > 55:
        print(f"   âœ“ Direction Accuracy {direction_accuracy:.2f}% > 55%: CÃ“ giÃ¡ trá»‹ thÆ°Æ¡ng máº¡i")
    elif direction_accuracy > 50:
        print(f"   ~ Direction Accuracy {direction_accuracy:.2f}%: HÆ¡i tá»‘t hÆ¡n ngáº«u nhiÃªn")
    else:
        print(f"   âœ— Direction Accuracy {direction_accuracy:.2f}% â‰¤ 50%: KHÃ”NG tá»‘t hÆ¡n ngáº«u nhiÃªn")
    
    print()
    
    return {
        "Model": model_name,
        "RMSE": rmse,
        "MAE": mae,
        "R2": r2,
        "Direction_Accuracy": direction_accuracy
    }


def create_lstm_sequences(data, features, target, lookback=10):
    """
    Táº¡o sequences cho LSTM
    
    LSTM cáº§n input dáº¡ng 3D: [samples, timesteps, features]
    - samples: Sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u
    - timesteps: Sá»‘ bÆ°á»›c thá»i gian nhÃ¬n láº¡i (lookback window)
    - features: Sá»‘ lÆ°á»£ng features
    
    Tham sá»‘:
    - data: DataFrame chá»©a features vÃ  target
    - features: List tÃªn cÃ¡c features
    - target: TÃªn cá»™t target
    - lookback: Sá»‘ ngÃ y nhÃ¬n láº¡i (default=10, cÃ³ thá»ƒ Ä‘iá»u chá»‰nh tá»« PACF)
    
    VÃ­ dá»¥:
    - Lookback=10: DÃ¹ng dá»¯ liá»‡u 10 ngÃ y trÆ°á»›c Ä‘á»ƒ dá»± bÃ¡o ngÃ y hÃ´m nay
    - Vá»›i 1000 ngÃ y data â†’ cÃ³ 990 samples (vÃ¬ 10 ngÃ y Ä‘áº§u khÃ´ng Ä‘á»§ lookback)
    """
    X, y = [], []
    
    for i in range(lookback, len(data)):
        # Láº¥y lookback ngÃ y trÆ°á»›c Ä‘Ã³
        X.append(data[features].iloc[i-lookback:i].values)
        # Target lÃ  giÃ¡ trá»‹ táº¡i thá»i Ä‘iá»ƒm i
        y.append(data[target].iloc[i])
    
    return np.array(X), np.array(y)


def run_modeling(train_file="train_data.csv", test_file="test_data.csv"):
    """
    Huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh dá»± bÃ¡o Log Returns
    
    THAY Äá»”I QUAN TRá»ŒNG so vá»›i version cÅ©:
    - Target: Log_Returns thay vÃ¬ Close price
    - KhÃ´ng dÃ¹ng inverse scaling (vÃ¬ returns khÃ´ng cáº§n scale back)
    - ThÃªm Direction Accuracy metric
    - Implement BiLSTM (Ä‘Ã£ bá» comment)
    - PhÃ¢n tÃ­ch residuals vá»›i Ljung-Box test
    
    MÃ´ hÃ¬nh:
    1. Linear Regression (Baseline)
    2. XGBoost (Tree-based, capture non-linearity)
    3. BiLSTM (Deep Learning for sequential data)
    """
    print("\n" + "="*80)
    print(" " * 25 + "MODELING - LOG RETURNS PREDICTION")
    print("="*80 + "\n")
    
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    data_processed_dir = os.path.join(base_dir, "data", "processed")
    results_dir = os.path.join(base_dir, "results", "figures")
    metrics_path = os.path.join(base_dir, "results", "metrics.csv")

    if train_file == "train_data.csv":
        train_file = os.path.join(data_processed_dir, "train_data.csv")
    if test_file == "test_data.csv":
        test_file = os.path.join(data_processed_dir, "test_data.csv")

    if not os.path.exists(train_file) or not os.path.exists(test_file):
        print("Lá»—i: KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u train/test.")
        return

    # 1. Load Data
    print("ğŸ“‚ Loading data...")
    train_df = pd.read_csv(train_file, index_col='Date', parse_dates=True)
    test_df = pd.read_csv(test_file, index_col='Date', parse_dates=True)
    print(f"   Train: {len(train_df)} samples")
    print(f"   Test:  {len(test_df)} samples\n")

    # === THAY Äá»”I QUAN TRá»ŒNG: Target lÃ  Log_Returns ===
    target = 'Log_Returns'
    
    # Features: Loáº¡i bá» cÃ¡c cá»™t khÃ´ng dÃ¹ng
    exclude_cols = [
        'Log_Returns',  # Target
        'Close',  # GiÃ¡ tuyá»‡t Ä‘á»‘i (khÃ´ng dÃ¹ng ná»¯a)
        'Outlier',  # Flag
        'Price_Direction',  # Target cho classification (dÃ¹ng riÃªng)
        'Open', 'High', 'Low', 'Volume'  # Raw values (Ä‘Ã£ cÃ³ derived features)
    ]
    features = [c for c in train_df.columns if c not in exclude_cols]
    
    print(f"ğŸ¯ Target: {target}")
    print(f"ğŸ“Š Features ({len(features)}): {features[:5]}... (showing first 5)\n")
    
    X_train = train_df[features]
    y_train = train_df[target]
    X_test = test_df[features]
    y_test = test_df[target]

    results = []
    predictions = pd.DataFrame(index=X_test.index)
    predictions['Actual_Returns'] = y_test

    # ========================================================================
    # 2. LINEAR REGRESSION (Baseline)
    # ========================================================================
    print("\n" + "â–ˆ"*70)
    print("MODEL 1: LINEAR REGRESSION (Baseline)")
    print("â–ˆ"*70)
    print("\nÄÃ¢y lÃ  baseline model Ä‘Æ¡n giáº£n nháº¥t.")
    print("Giáº£ Ä‘á»‹nh: Má»‘i quan há»‡ tuyáº¿n tÃ­nh giá»¯a features vÃ  returns")
    print("Huáº¥n luyá»‡n...\n")
    
    lr_model = LinearRegression()
    lr_model.fit(X_train, y_train)
    y_pred_lr = lr_model.predict(X_test)
    predictions['LR_Returns'] = y_pred_lr
    
    lr_metrics = evaluate_returns_metrics(y_test, y_pred_lr, "Linear Regression")
    results.append(lr_metrics)

    # ========================================================================
    # 3. XGBOOST
    # ========================================================================
    print("\n" + "â–ˆ"*70)
    print("MODEL 2: XGBOOST (Gradient Boosting)")
    print("â–ˆ"*70)
    print("\nXGBoost lÃ  ensemble of decision trees.")
    print("Æ¯u Ä‘iá»ƒm: Capture non-linearity, feature importance, robust to outliers")
    print("Tham sá»‘:")
    print("  - n_estimators=1000: Sá»‘ cÃ¢y quyáº¿t Ä‘á»‹nh")
    print("  - learning_rate=0.01: Tá»‘c Ä‘á»™ há»c (tháº¥p = há»c cháº­m nhÆ°ng á»•n Ä‘á»‹nh)")
    print("  - max_depth=5: Äá»™ sÃ¢u tá»‘i Ä‘a cá»§a má»—i cÃ¢y (trÃ¡nh overfitting)")
    print("Huáº¥n luyá»‡n...\n")
    
    xgb_model = xgb.XGBRegressor(
        n_estimators=1000,
        learning_rate=0.01,
        max_depth=5,
        subsample=0.8,
        colsample_bytree=0.8,
        objective='reg:squarederror',
        random_state=42
    )
    
    xgb_model.fit(
        X_train, y_train,
        eval_set=[(X_test, y_test)],
        verbose=False
    )
    
    y_pred_xgb = xgb_model.predict(X_test)
    predictions['XGBoost_Returns'] = y_pred_xgb
    
    xgb_metrics = evaluate_returns_metrics(y_test, y_pred_xgb, "XGBoost")
    results.append(xgb_metrics)
    
    # Feature Importance
    print("\nğŸ“Š Váº½ Feature Importance...")
    plt.figure(figsize=(12, 8))
    xgb.plot_importance(xgb_model, importance_type='weight', max_num_features=15,
                        title='XGBoost Feature Importance (Top 15)\nÄá»™ quan trá»ng cá»§a tá»«ng feature trong viá»‡c dá»± bÃ¡o Log Returns')
    plt.xlabel('F score (sá»‘ láº§n feature Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ split)', fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, 'feature_importance_returns.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print("   â†’ ÄÃ£ lÆ°u: feature_importance_returns.png")
    
    print("\nğŸ“ NHáº¬N XÃ‰T Feature Importance:")
    feature_importance = xgb_model.get_booster().get_score(importance_type='weight')
    top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:5]
    for i, (feat, score) in enumerate(top_features, 1):
        # Map feature index to name
        feat_idx = int(feat[1:]) if feat.startswith('f') else -1
        feat_name = features[feat_idx] if 0 <= feat_idx < len(features) else feat
        print(f"   {i}. {feat_name}: {score:.0f} láº§n sá»­ dá»¥ng")
    
    # ========================================================================
    # 4. BiLSTM (Bidirectional LSTM)
    # ========================================================================
    print("\n" + "â–ˆ"*70)
    print("MODEL 3: BiLSTM (Bidirectional Long Short-Term Memory)")
    print("â–ˆ"*70)
    print("\nBiLSTM lÃ  mÃ´ hÃ¬nh Deep Learning cho dá»¯ liá»‡u chuá»—i thá»i gian.")
    print("Æ¯u Ä‘iá»ƒm:")
    print("  - Capture long-term dependencies (phá»¥ thuá»™c dÃ i háº¡n)")
    print("  - Bidirectional: há»c tá»« cáº£ quÃ¡ khá»© VÃ€ tÆ°Æ¡ng lai")
    print("  - Tá»± Ä‘á»™ng há»c features tá»« sequences")
    print("\nCáº¥u trÃºc:")
    print("  - Layer 1: BiLSTM(64 units) + Dropout(0.2)")
    print("  - Layer 2: BiLSTM(32 units) + Dropout(0.2)")
    print("  - Layer 3: Dense(16, relu)")
    print("  - Output: Dense(1) - dá»± bÃ¡o Log Return")
    print("\nChuáº©n bá»‹ sequences (lookback=10 ngÃ y)...\n")
    
    # Táº¡o sequences
    lookback = 10  # CÃ³ thá»ƒ Ä‘iá»u chá»‰nh dá»±a trÃªn PACF analysis
    
    X_train_lstm, y_train_lstm = create_lstm_sequences(
        train_df, features, target, lookback
    )
    X_test_lstm, y_test_lstm = create_lstm_sequences(
        test_df, features, target, lookback
    )
    
    print(f"   LSTM Train shape: {X_train_lstm.shape}")
    print(f"   LSTM Test shape:  {X_test_lstm.shape}")
    print(f"   Format: (samples, timesteps={lookback}, features={len(features)})\n")
    
    # Build model
    print("XÃ¢y dá»±ng BiLSTM model...")
    lstm_model = Sequential([
        Bidirectional(LSTM(64, return_sequences=True), 
                     input_shape=(lookback, len(features))),
        Dropout(0.2),
        Bidirectional(LSTM(32)),
        Dropout(0.2),
        Dense(16, activation='relu'),
        Dense(1)  # Output: Log Returns
    ])
    
    lstm_model.compile(
        optimizer='adam',
        loss='mse',
        metrics=['mae']
    )
    
    print(lstm_model.summary())
    
    # Train vá»›i EarlyStopping
    print("\nHuáº¥n luyá»‡n BiLSTM (cÃ³ thá»ƒ máº¥t vÃ i phÃºt)...")
    es = EarlyStopping(
        monitor='val_loss',
        patience=15,
        restore_best_weights=True,
        verbose=1
    )
    
    history = lstm_model.fit(
        X_train_lstm, y_train_lstm,
        validation_split=0.2,
        epochs=100,
        batch_size=32,
        callbacks=[es],
        shuffle=False,  # Äáº£m báº£o reproducibility
        verbose=1
    )
    
    # Predict
    y_pred_lstm = lstm_model.predict(X_test_lstm, verbose=0).flatten()
    
    # Add to predictions (align indices)
    predictions['BiLSTM_Returns'] = np.nan
    predictions.iloc[lookback:lookback+len(y_pred_lstm), 
                    predictions.columns.get_loc('BiLSTM_Returns')] = y_pred_lstm
    
    lstm_metrics = evaluate_returns_metrics(y_test_lstm, y_pred_lstm, "BiLSTM")
    results.append(lstm_metrics)
    
    # Plot training history
    print("\nğŸ“Š Váº½ Training History...")
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)
    axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)
    axes[0].set_title('BiLSTM Training History - Loss', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('Epoch', fontweight='bold')
    axes[0].set_ylabel('MSE Loss', fontweight='bold')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2)
    axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)
    axes[1].set_title('BiLSTM Training History - MAE', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('Epoch', fontweight='bold')
    axes[1].set_ylabel('MAE', fontweight='bold')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, 'lstm_training_history.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print("   â†’ ÄÃ£ lÆ°u: lstm_training_history.png")

    # ========================================================================
    # 5. COMPARE MODELS
    # ========================================================================
    print("\n" + "="*80)
    print("SO SÃNH MÃ” HÃŒNH")
    print("="*80 + "\n")
    
    results_df = pd.DataFrame(results)
    print(results_df.to_markdown(index=False))
    results_df.to_csv(metrics_path, index=False)
    print(f"\nâœ“ ÄÃ£ lÆ°u metrics vÃ o: {metrics_path}")

    # ========================================================================
    # 6. VISUALIZATION: Actual vs Predicted Returns
    # ========================================================================
    print("\nğŸ“Š Váº½ biá»ƒu Ä‘á»“ so sÃ¡nh Actual vs Predicted Returns...")
    
    fig, axes = plt.subplots(3, 1, figsize=(16, 12))
    
    # Zoom last 100 days
    subset = predictions.iloc[-100:]
    
    # Plot 1: Linear Regression
    axes[0].plot(subset.index, subset['Actual_Returns'], label='Actual Returns', 
                color='black', linewidth=2, alpha=0.7)
    axes[0].plot(subset.index, subset['LR_Returns'], label='LR Prediction', 
                color='#E63946', linestyle='--', linewidth=1.5)
    axes[0].axhline(y=0, color='gray', linestyle=':', alpha=0.5)
    axes[0].set_title('Linear Regression: Actual vs Predicted Log Returns (Last 100 Days)', 
                     fontsize=13, fontweight='bold')
    axes[0].set_ylabel('Log Returns', fontweight='bold')
    axes[0].legend(loc='upper left')
    axes[0].grid(True, alpha=0.3)
    
    # Plot 2: XGBoost
    axes[1].plot(subset.index, subset['Actual_Returns'], label='Actual Returns', 
                color='black', linewidth=2, alpha=0.7)
    axes[1].plot(subset.index, subset['XGBoost_Returns'], label='XGBoost Prediction', 
                color='#2A9D8F', linestyle='--', linewidth=1.5)
    axes[1].axhline(y=0, color='gray', linestyle=':', alpha=0.5)
    axes[1].set_title('XGBoost: Actual vs Predicted Log Returns (Last 100 Days)', 
                     fontsize=13, fontweight='bold')
    axes[1].set_ylabel('Log Returns', fontweight='bold')
    axes[1].legend(loc='upper left')
    axes[1].grid(True, alpha=0.3)
    
    # Plot 3: BiLSTM
    subset_lstm = subset.dropna(subset=['BiLSTM_Returns'])
    axes[2].plot(subset_lstm.index, subset_lstm['Actual_Returns'], label='Actual Returns', 
                color='black', linewidth=2, alpha=0.7)
    axes[2].plot(subset_lstm.index, subset_lstm['BiLSTM_Returns'], label='BiLSTM Prediction', 
                color='#F4A261', linestyle='--', linewidth=1.5)
    axes[2].axhline(y=0, color='gray', linestyle=':', alpha=0.5)
    axes[2].set_title('BiLSTM: Actual vs Predicted Log Returns (Last 100 Days)', 
                     fontsize=13, fontweight='bold')
    axes[2].set_xlabel('Date', fontweight='bold')
    axes[2].set_ylabel('Log Returns', fontweight='bold')
    axes[2].legend(loc='upper left')
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, 'model_comparison_returns.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print("   â†’ ÄÃ£ lÆ°u: model_comparison_returns.png")
    
    # ========================================================================
    # 7. RESIDUALS ANALYSIS
    # ========================================================================
    print("\n" + "â–ˆ"*70)
    print("PHÃ‚N TÃCH RESIDUALS (White Noise Test)")
    print("â–ˆ"*70 + "\n")
    
    from statistical_tests import StatisticalTests
    tester = StatisticalTests(results_dir=results_dir)
    
    # Test residuals for each model
    for model_name, pred_col in [('Linear Regression', 'LR_Returns'),
                                   ('XGBoost', 'XGBoost_Returns'),
                                   ('BiLSTM', 'BiLSTM_Returns')]:
        residuals = predictions['Actual_Returns'] - predictions[pred_col]
        residuals = residuals.dropna()
        
        if len(residuals) > 20:
            tester.ljung_box_test(residuals, lags=10, name=model_name)
    
    # ========================================================================
    # 8. SAVE PREDICTIONS
    # ========================================================================
    predictions_path = os.path.join(base_dir, "results", "predictions_returns.csv")
    predictions.to_csv(predictions_path)
    print(f"\nâœ“ ÄÃ£ lÆ°u predictions vÃ o: {predictions_path}")

    print("\n" + "="*80)
    print(" " * 30 + "MODELING HOÃ€N THÃ€NH")
    print("="*80 + "\n")
    
    print("ğŸ“ TÃ“M Táº®T:")
    print("   - ÄÃ£ chuyá»ƒn tá»« dá»± bÃ¡o GIÃ TUYá»†T Äá»I sang Dá»° BÃO LOG RETURNS")
    print("   - RÂ² tháº¥p (< 0.1) lÃ  BÃŒN THÆ¯á»œNG vá»›i dá»¯ liá»‡u tÃ i chÃ­nh")
    print("   - Direction Accuracy > 55% = CÃ“ giÃ¡ trá»‹ thÆ°Æ¡ng máº¡i")
    print("   - Residuals analysis kiá»ƒm tra xem mÃ´ hÃ¬nh cÃ³ bá» sÃ³t thÃ´ng tin khÃ´ng")
    print()


if __name__ == "__main__":
    run_modeling()
